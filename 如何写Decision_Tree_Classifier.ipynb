{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 目标是什么？ \n",
    "* 最终目标是建立一个决策树的classifier， 所使用的算法是 **Classification and Regression Tree (CART)**.\n",
    "\n",
    "\n",
    "* 目标细分为：\n",
    "\n",
    "    1. 将classifier 作的尽量普遍化， 可以适合各种数据处理，不仅仅是书上的例子。\n",
    "    2. 将classifier 封装在一个class 里面， 最主要的method 包括训练（fit）， 预测（predict）。\n",
    "\n",
    "## 2. 前提是什么？\n",
    "1. 需要真正了解什么是决策树\n",
    "\n",
    "\n",
    "2. ID3, C4.5， CART 的不同之处\n",
    "\n",
    "    * CART 是最普遍使用的， 可以用来作regression 和 classification。 二叉树\n",
    "    * ID3  已经过时， 只能作classification. 多叉数\n",
    "    * C4.5 ID3 的升级版， 可以作classification 和 regression。 \n",
    " \n",
    " \n",
    "3. 建立决策树时所使用的衡量单位（gini impurity, entropy). \n",
    "\n",
    "    * ID3 C4.5 ----> entropy\n",
    "    * CART ------> gini impurity\n",
    "    * 这个并不是绝对的， 所以我们将把两种metric 都包括在class 里面， 把metric 的选择作为一个hyperparameter\n",
    "    \n",
    "    \n",
    "4. 树结构需要使用编程中递归的概念， 所以需要了解一些递归的知识。\n",
    "\n",
    "\n",
    "## 3. 如何创建一个class （我个人的理解）：\n",
    "无论是什么样子的class， 无非就包括两个东西：\n",
    "\n",
    "1. variable 变量\n",
    "2. method (or function） 函数\n",
    "\n",
    "我们可以把变量想象成名词， 函数想象成动词。 比如我们有一个class 叫做 Car。 那么 车的名字， 型号 就是他的变量， 发动， 停止就是函数。 \n",
    "\n",
    "\n",
    "## 4. 那么创建这个决策树的class， 我们具体都需要什么：\n",
    "1. 起初， 这个世界没有数。 只有一堆数据。 这堆数据可以看成是节点（node).\n",
    "1. 向数据提出一个问题。\n",
    "2. 根据问题的结果（是或者不是）， 从而将数据一分为二。 --> 从一个node 变成了两个 node。\n",
    "3. 使用info_gain  来判断此分类是否有效。 而info_gain 是通过分前的节点（node）和分后的两个节点（node） 的impurity 或者 entropy的相差而得来的。\n",
    "4. 经过多次验证， 我们终于得到了最好的info_gain, 同时也知道了什么问题在这个时候该问， 什么时候不该问。\n",
    "5. 最终， 分裂后的节点里面只包括一种类别， 这个节点就不能再分裂了， 我们叫这个节点 leaf（叶子). \n",
    "6. 所以叶子就是不能再分裂了， 而node 还可以继续分裂。 一定要知道他们的不同之处。 leaf 和 node 共同组成了 decision tree。 Leaf 所包含的信息就是我们所需要的prediction。 \n",
    "\n",
    "![Decision_Tree](img/decision_tree.png)\n",
    "\n",
    "\n",
    "## 5. 把第4步中我们所考虑到的所有东西， 按照名词和动词来分类：\n",
    "* 变量 (名词）：\n",
    "    1. Node （节点）\n",
    "    2. Leaf （叶子）\n",
    "    3. Question （问题）\n",
    "    \n",
    "* 函数 (动词）：\n",
    "    1. split (分裂节点）\n",
    "    2. ask question （问问题来判断）\n",
    "    3. get_metric (计算 impurity 或者 entropy）\n",
    "    4. get_info_gain (计算 info_gain)\n",
    "    5. get_best_split (获得最好的分裂）\n",
    "\n",
    "## 6. 一些注意事项：\n",
    "\n",
    "1. 既然是用python来写， 尽量遵循PEP8\n",
    "2. 命名要简单， 明了。 \n",
    "3. 尽量写出动态的程序， 不要hardcode （以上三点， 实战的书绝对是一个极端反面教材）。\n",
    "3. （best practices if you are intermediate in Python）先按照上面的分析， 把整个class 的框架写出来， 然后写测试程序， 最后再写class。 通过写测试， 会让你的整个code 的结构印象更加深刻， 同时后写class 过程中的 debug 提供特别多的帮助。\n",
    "4. 在写整个框架的时候， 需要把docstring 也写上去， 方便你对整个code 的把控。\n",
    "5. 除了docstring， 普通的comment不要太多。 要用code 本身去述说， 而不是你的comment。\n",
    "6. 对重要的一些函数里的变量要提出硬性的规定， 比如在训练模型的时候， 输入的数据必须是numpy array， 或者必须是list， 这样你初期就不需要为这些数据结构的转换而分心。\n",
    "7. 命名的时候， 要前后一致。 比如我的命名方法是： X 表示的是feature矩阵， x 表示的是feature向量， y 表示 lable 向量。 也就是说，大写字母表示矩阵， 小写字母表示向量。 X 表示 feature， y 表示 label。\n",
    "8. 抽象原则（The principle of Abstraction)， 参考https://www.jianshu.com/p/683ae1bc87f0\n",
    "\n",
    "## 7. 以下是我按照写的决策树的class的整体结构。 希望对你有帮助。 \n",
    "你也可以按照上述的思路，写出一个不同的大体结构， 这都无所谓。并且这个初期结构也并不一定是最终的class的样子。写程序是一个边写，边测试，便改正的过程。 但是初期的整体结构绝对对你的思路和code的把握至关重要。千万不要一言不和， 就不经过思考， 直接写code。磨刀不误砍柴工。\n",
    "\n",
    "我也写了一个简单的decision tree classifier的最终版本。 你们也可以当作参考。 \n",
    "https://github.com/superyang713/MachineLearningInAction-Camp/blob/master/Week2/lib/tree.py\n",
    "\n",
    "下面是使用我写的DecisionTreeClassifier 的例子。同时你也可以看到决策树和knn之间的不同之处。\n",
    "https://github.com/superyang713/MachineLearningInAction-Camp/blob/master/Week2/decision_tree_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"\n",
    "    A node is splitted based on the question asked.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_number, feature):\n",
    "        self.column_number = column_number\n",
    "        self.feature = feature\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Some way to represent a question.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def ask(self, row):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            bool\n",
    "        \"\"\"  \n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Leaf:\n",
    "    \"\"\"\n",
    "    The end node (where no more info is gained). It should contain the predictions we want with only one type of label.\n",
    "    \"\"\"\n",
    "    def __init__(self, y):\n",
    "        self.prediction = self.y[0]\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Some way to represent a leaf.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _label_counts(y):\n",
    "        \"\"\"\n",
    "        Counts the number of the samples that have the same label.\n",
    "        Return:\n",
    "            counts: dict\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A part of the tree. It is used to structure the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "    \n",
    "    \n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision tree classifier based on CART algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, metric=\"gini\"):\n",
    "        self.metric_type = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree, in other words, train the model, and pass it to the class variable.\n",
    "        Recursion should be used with multiple returns. Return should be either a leaf or a node.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def print_tree(self):\n",
    "        \"\"\"\n",
    "        It is a helper method to visualize the tree if the model is small. \n",
    "        \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _split(X, y, question):\n",
    "        \"\"\"\n",
    "        A private static helper function used to split the node into two nodes based on a question.\n",
    "        It is a static method because it does not need  the access to class variables.\n",
    "        But for the completeness of the algorithm, it would be better to still put it inside the class.\n",
    "            \n",
    "        Returns:\n",
    "            true_branch: np array\n",
    "            false_branch: np array\n",
    "        \"\"\"\n",
    "        pass\n",
    "            \n",
    "    def _get_metric(self, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            gini or entropy: float\n",
    "                a metric used in the information gain.\n",
    "        \"\"\"\n",
    "            \n",
    "        if self.metric_type == 'gini':\n",
    "            pass\n",
    "        if self.metric_type == 'entropy':\n",
    "            pass\n",
    "        \n",
    "    def _get_info_gain(self, y, true_branch, false_branch):\n",
    "        \"\"\"\n",
    "        Calculate the info gain after a node is splitted into two more nodes.\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'gini':\n",
    "            pass\n",
    "        if self.metric_type == 'entropy':\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            best_question: an instance of Question that get the highest info gain. We use it to split the tree.\n",
    "            best_gain: the info gain when the best question is asked.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work on individual pieces.\n",
    "\n",
    "Now we have the basic structure of the code, and most importantly, the big picture of how to build the decision tree, now it's time to work on small pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, make a test dataset for code testing.\n",
    "\n",
    "The dataset looks like this:\n",
    "\n",
    "```\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "The first feature is color, and the second is radius.\n",
    "The third column is label.\n",
    "\n",
    "However, since we are going to work with numpy array, it would be better to convert the categorical feature into nominal numbers.\n",
    "\n",
    "**For the color feature,**\n",
    "\n",
    "* 1 --> 'green'\n",
    "* 2 --> 'yellow'\n",
    "* 3 --> 'red'\n",
    "   \n",
    "**For the label,**\n",
    "\n",
    "* 1 --> 'apple'\n",
    "* 2 --> 'grape'\n",
    "* 3 --> 'lemon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array([\n",
    "    [1, 3, 1],\n",
    "    [2, 3, 1],\n",
    "    [3, 1, 2],\n",
    "    [3, 1, 2],\n",
    "    [2, 3, 3],\n",
    "])\n",
    "\n",
    "X = training_data[:, :-1]\n",
    "y = training_data[:, -1]\n",
    "\n",
    "# A list or numpy array does not have a header, \n",
    "# so for demonstration purpose, I manually added a header.\n",
    "header = [\"color\", \"diameter\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 3],\n",
       "       [3, 1],\n",
       "       [3, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Question class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"\n",
    "    A node is splitted based on the question asked.\n",
    "    \"\"\"\n",
    "    def __init__(self, column_number, threshold):\n",
    "        self.column_number = column_number\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Is {} > {}\".format(\n",
    "            header[self.column_number], self.threshold\n",
    "        )\n",
    "    \n",
    "    def ask(self, x):\n",
    "        feature = x[self.column_number]\n",
    "        return feature > self.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is:  Is diameter > 1\n",
      "For the first test sample, the answer is:  True\n",
      "For the second test sample, the answer is:  False\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "\n",
    "sample_1 = training_data[1, :]\n",
    "sample_2 = training_data[2, :]\n",
    "q = Question(1, 1)\n",
    "print('The question is: ', q)\n",
    "print('For the first test sample, the answer is: ', q.ask(sample_1))\n",
    "print('For the second test sample, the answer is: ', q.ask(sample_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Leaf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    \"\"\"\n",
    "    The end node (where no more info is gained). It should contain the predictions we want with only one type of label.\n",
    "    \"\"\"\n",
    "    def __init__(self, y):\n",
    "        self.prediction = self._label_counts(y)\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.prediction)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _label_counts(y):\n",
    "        \"\"\"\n",
    "        Counts the number of the samples that have the same label.\n",
    "        Return:\n",
    "            counts: dict\n",
    "        \"\"\"\n",
    "        \n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return unique[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This leaf contains the following prediction:  3\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "\n",
    "labels = np.array([3, 3, 1, 2, 1, 3])\n",
    "leaf = Leaf(labels)\n",
    "print('This leaf contains the following prediction: ', leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Node class.\n",
    "\n",
    "**Actually it is so simple that the class has already be completed during the structuring process.**\n",
    "\n",
    "A little bit more explanation:\n",
    "1. We need the question in the Node, because the question is more like a guide, pointing where the data should flow. (think about tensorflow).\n",
    "\n",
    "2. Every node has a true branch, and a false branch. Again, whether data flows into a true branch or a false branch all depends on the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A part of the tree. It is used to structure the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, question, true_node, false_node):\n",
    "        self.question = question\n",
    "        self.true_node = true_node\n",
    "        self.false_node = false_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the hardest part, let's build the classifier:\n",
    "Because it is a big class, you should build it step by step, following the order that we have come up with when we were structuring the code.\n",
    "\n",
    "1. Split data\n",
    "3. metric\n",
    "4. info_gain \n",
    "5. get_best_split\n",
    "\n",
    "All the functions above will be part of the model training process, which is the `fit` method in the class.\n",
    "\n",
    "Then, you can work on the following two methods:\n",
    "\n",
    "1. visualization --> a way to visualize the decision tree, and test the trained model.\n",
    "2. predict --> a way to predict new instance based on the trained model.\n",
    "\n",
    "\n",
    "**Remember, after you finish each method, you should test it to make sure it actually works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Decision tree classifier based on CART algorithm.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, metric=\"gini\"):\n",
    "        # hyperparameter\n",
    "        self.metric_type = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the tree, in other words, train the model, and pass it to the class variable.\n",
    "        Recursion should be used with multiple returns. Return should be either a leaf or a node.\n",
    "        \"\"\"\n",
    "        gain, question = self._find_best_split(X, y)\n",
    "        \n",
    "        if gain == 0:\n",
    "            return Leaf(y)\n",
    "        \n",
    "        X_true_branch, y_true_branch, X_false_branch, y_false_branch = self._split(X, y, question)\n",
    "        \n",
    "        true_node = self.fit(X_true_branch, y_true_branch)\n",
    "        false_node = self.fit(X_false_branch, y_false_branch)\n",
    "        self.tree = Node(question, true_node, false_node)\n",
    "        return self.tree\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)                                                                                                                                                         \n",
    "        y_predict = np.empty(n_samples)                                                                                                                                            \n",
    "                                                                                                                                                                                   \n",
    "        for i, x in enumerate(X):                                                                                                                                                  \n",
    "            y_predict[i] = self._predict(self.tree, x)                                                                                                                             \n",
    "                                                                                                                                                                                   \n",
    "        return y_predict      \n",
    "        \n",
    "    def _predict(self, tree, x):\n",
    "        \"\"\"\n",
    "        Use the model to predict the result with the new data X.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(tree, Leaf):\n",
    "            return tree.prediction\n",
    "\n",
    "        if tree.question.ask(x):\n",
    "            return self._predict(tree.true_node, x)\n",
    "        else:\n",
    "            return self._predict(tree.false_node, x)\n",
    "          \n",
    "    def print_tree(self, spacing=''):\n",
    "        self._print_tree(self.tree, spacing=spacing)\n",
    "            \n",
    "            \n",
    "    def _print_tree(self, tree, spacing=''):\n",
    "        \"\"\"\n",
    "        It is a helper method to visualize the tree if the model is small. Used to test model. \n",
    "        \"\"\"\n",
    "        if isinstance(tree, Leaf):\n",
    "            print(spacing + \"Predict\", tree.prediction)\n",
    "            return\n",
    "\n",
    "        # Print the question at this node\n",
    "        print(spacing + str(tree.question))\n",
    "\n",
    "        # Call this function recursively on the true branch\n",
    "        print(spacing + '--> True:')\n",
    "        self._print_tree(tree.true_node, spacing + \"  \")\n",
    "\n",
    "        # Call this function recursively on the false branch\n",
    "        print(spacing + '--> False:')\n",
    "        self._print_tree(tree.false_node, spacing + \"  \")\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _split(X, y, question):\n",
    "        \"\"\"\n",
    "        A private static helper function used to split the node into two nodes based on a question.\n",
    "        It is a static method because it does not need  the access to class variables.\n",
    "        But for the completeness of the algorithm, it would be better to still put it inside the class.\n",
    "            \n",
    "        Returns:\n",
    "            true_branch: np array\n",
    "            false_branch: np array\n",
    "        \"\"\"\n",
    "        data = np.hstack((X, y.reshape(-1, 1)))\n",
    "        true_branch = np.array([])\n",
    "        false_branch = np.array([])\n",
    "        for row in data:\n",
    "            if question.ask(row):\n",
    "                true_branch = np.vstack((true_branch, row)) if true_branch.size else row.reshape(1, -1)\n",
    "            else:\n",
    "                false_branch = np.vstack((false_branch, row)) if false_branch.size else row.reshape(1, -1)\n",
    "        \n",
    "        if len(true_branch) == 0 or len(false_branch) == 0:\n",
    "            X_true_branch, y_true_branch, X_false_branch, y_false_branch = [], [], [], []\n",
    "        else:\n",
    "            X_true_branch = true_branch[:, :-1]\n",
    "            y_true_branch = true_branch[:, -1]\n",
    "            X_false_branch = false_branch[:, :-1]\n",
    "            y_false_branch = false_branch[:, -1]\n",
    "        \n",
    "        return X_true_branch, y_true_branch, X_false_branch, y_false_branch\n",
    "            \n",
    "    def _get_metric(self, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            gini or entropy: float\n",
    "                a metric used in the information gain.\n",
    "        \"\"\"\n",
    "        n_samples = len(y)\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        if self.metric_type == 'gini':\n",
    "            impurity = 1\n",
    "            for count in counts:\n",
    "                probability = float(count / n_samples)\n",
    "                impurity -= probability ** 2\n",
    "            return impurity\n",
    "                \n",
    "        if self.metric_type == 'entropy':\n",
    "            entropy = 0\n",
    "            for count in counts:\n",
    "                probability = float(count / n_samples)\n",
    "                entropy -= probability * np.log2(probability)\n",
    "            return entropy\n",
    "            \n",
    "        \n",
    "    def _get_info_gain(self, y, y_true_branch, y_false_branch):\n",
    "        \"\"\"\n",
    "        Calculate the info gain after a node is splitted into two more nodes.\n",
    "        \"\"\"\n",
    "        p_true = len(y_true_branch) / len(y)\n",
    "        p_false = 1 - p_true\n",
    "        \n",
    "        metric_before_split = self._get_metric(y)\n",
    "        metric_true = self._get_metric(y_true_branch)\n",
    "        metric_false = self._get_metric(y_false_branch)\n",
    "        \n",
    "        gain = metric_before_split - p_true * metric_true - p_false * metric_false\n",
    "        \n",
    "        return gain\n",
    "        \n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            best_question: an instance of Question that get the highest info gain. \n",
    "                We use it to split the tree.\n",
    "            best_gain: the info gain when the best question is asked.\n",
    "        \"\"\"\n",
    "        best_gain = 0\n",
    "        best_question = None\n",
    "        n_features = len(X[0])\n",
    "        \n",
    "        for column_number in range(n_features):\n",
    "            thresholds = set([x[column_number] for x in X])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                question = Question(column_number, threshold)         \n",
    "                \n",
    "                X_true_branch, y_true_branch, X_false_branch, y_false_branch = self._split(X, y, question)\n",
    "                \n",
    "                if len(X_true_branch) == 0 or len(X_false_branch) == 0:\n",
    "                    continue\n",
    "                \n",
    "                gain = self._get_info_gain(y, y_true_branch, y_false_branch)\n",
    "                \n",
    "                if gain >= best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_question = question\n",
    "        \n",
    "        return best_gain, best_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test each method in DecisionTreeClassifier class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Test `_split` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_true_branch is <class 'numpy.ndarray'> \n",
      "[[1 3]\n",
      " [2 3]\n",
      " [2 3]]\n",
      "-------------------------\n",
      "y_true_branch is <class 'numpy.ndarray'> \n",
      "[1 1 3]\n",
      "-------------------------\n",
      "X_false_branch is <class 'numpy.ndarray'> \n",
      "[[3 1]\n",
      " [3 1]]\n",
      "-------------------------\n",
      "y_false_branch is <class 'numpy.ndarray'> \n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "question = Question(1, 1)\n",
    "X_true_branch, y_true_branch, X_false_branch, y_false_branch = decision._split(X, y, question)\n",
    "print('X_true_branch is {} \\n{}'.format(type(X_true_branch), X_true_branch))\n",
    "print('-------------------------')\n",
    "print('y_true_branch is {} \\n{}'.format(type(y_true_branch), y_true_branch))\n",
    "print('-------------------------')\n",
    "print('X_false_branch is {} \\n{}'.format(type(X_false_branch), X_false_branch))\n",
    "print('-------------------------')\n",
    "print('y_false_branch is {} \\n{}'.format(type(y_false_branch), y_false_branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test `_get_metric` method:\n",
    "$$\n",
    "impurity = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested impurities are \n",
      "0.0\n",
      "0.5\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "impurity_1 = decision._get_metric(np.array([1, 1, 1, 1, 1]))\n",
    "impurity_2 = decision._get_metric(np.array([1, 1, 2, 2]))\n",
    "impurity_3 = decision._get_metric(np.array([1, 1, 0, 0, 0]))\n",
    "print('The tested impurities are \\n{}\\n{}\\n{}'.format(\n",
    "    impurity_1, impurity_2, impurity_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "entropy = - \\sum_{i=1}^{n} p_i {\\log_2 p_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tested impurities are \n",
      "0.0\n",
      "1.0\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='entropy')\n",
    "\n",
    "impurity_1 = decision._get_metric(np.array([1, 1, 1, 1, 1]))\n",
    "impurity_2 = decision._get_metric(np.array([1, 1, 2, 2]))\n",
    "impurity_3 = decision._get_metric(np.array([1, 1, 0, 0, 0]))\n",
    "print('The tested impurities are \\n{}\\n{}\\n{}'.format(\n",
    "    impurity_1, impurity_2, impurity_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test `_get_info_gain` method:\n",
    "$$\n",
    "gain = metric_a - \\sum_{i=1}^{n} p_i * metric_b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The info gain is 0.37333333333333324\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "_, y_true_branch, _, y_false_branch = decision._split(X, y, q)\n",
    "\n",
    "gain = decision._get_info_gain(y, y_true_branch, y_false_branch)\n",
    "print('The info gain is {}'.format(gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Test `_find_best_split` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the main node:\n",
      "0.37333333333333324\n",
      "Is diameter > 1\n",
      "-----------------\n",
      "after the first split\n",
      "The best gain for the true branch: 0.11111111111111116\n",
      "The best gain for the false branch: 0\n",
      "The best question for the true branch is: Is color > 1 \n",
      "The best question for the false branch is: None \n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "best_gain, best_question = decision._find_best_split(X, y)\n",
    "print('For the main node:')\n",
    "print(best_gain)\n",
    "print(best_question)\n",
    "print('-----------------')\n",
    "print('after the first split')\n",
    "\n",
    "X_true_branch, y_true_branch, X_false_branch, y_false_branch = decision._split(X, y, question)\n",
    "best_gain_true_branch, best_question_true_branch = decision._find_best_split(X_true_branch, y_true_branch)\n",
    "best_gain_false_branch, best_question_false_branch = decision._find_best_split(X_false_branch, y_false_branch)\n",
    "\n",
    "print('The best gain for the true branch: {}'.format(best_gain_true_branch))\n",
    "print('The best gain for the false branch: {}'.format(best_gain_false_branch))\n",
    "print('The best question for the true branch is: {} '.format(best_question_true_branch))\n",
    "print('The best question for the false branch is: {} '.format(best_question_false_branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Test `fit` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter > 1\n",
      "--> True:\n",
      "  Is color > 1\n",
      "  --> True:\n",
      "    Predict 1\n",
      "  --> False:\n",
      "    Predict 1\n",
      "--> False:\n",
      "  Predict 2\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "decision.fit(X, y)\n",
    "decision.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diameter > 1\n",
      "--> True:\n",
      "  Is color > 1\n",
      "  --> True:\n",
      "    Predict 1\n",
      "  --> False:\n",
      "    Predict 1\n",
      "--> False:\n",
      "  Predict 2\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='entropy')\n",
    "\n",
    "decision.fit(X, y)\n",
    "decision.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 2. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "decision = DecisionTreeClassifier(metric='gini')\n",
    "\n",
    "decision.fit(X, y)\n",
    "y_predict = decision.predict(X)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
